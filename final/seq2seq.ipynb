{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "import math\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Input, Activation, Dot, LSTM, Dense, Embedding, Bidirectional, Lambda, Flatten, RepeatVector, Reshape, Permute, Concatenate, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parameter setting\n",
    "\n",
    "train = pd.read_csv('data/all/train.csv', header=None)\n",
    "test = pd.read_csv('data/all/test.csv', header=None)\n",
    "vocab_size = 30000\n",
    "latent_dim = 256\n",
    "embedding_dim = 256\n",
    "batch_size=128\n",
    "epochs = 20\n",
    "keep_prob = 0.5\n",
    "input_max_length = 69\n",
    "output_max_length = 25\n",
    "attention = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function definition\n",
    "\n",
    "def to_sequence(word_array, vocab):\n",
    "    return list(map(lambda word: vocab[word] if vocab[word] <= vocab_size else vocab['unk'] , word_array))\n",
    "def to_sentence(sequence, reverse_vocab):\n",
    "    if vocab['EOS'] in sequence: length = np.where(sequence == vocab['EOS'])[0][0]\n",
    "    return list(map(lambda index: reverse_vocab[index], sequence[:length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = list(train[0].values)\n",
    "texts += list(train[1].values)\n",
    "texts += list(test[0].values)\n",
    "texts = list(map(lambda x: x.split(' '), texts))\n",
    "\n",
    "vocab_freq = defaultdict(int)\n",
    "vocab = None\n",
    "reverse_vocab = None\n",
    "\n",
    "for row in texts:\n",
    "    for word in row:\n",
    "        vocab_freq[word] += 1\n",
    "        \n",
    "vocab = sorted(vocab_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "vocab = dict([('unk', -1)] + vocab)\n",
    "cnt = 1\n",
    "for key, value in vocab.items():\n",
    "    vocab[key] = cnt\n",
    "    cnt += 1\n",
    "reverse_vocab = dict([(v, k) for k, v in vocab.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec(texts, size=256, window=5, min_count=5, workers=4)\n",
    "# model.save('model/w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = Word2Vec.load('model/w2v')\n",
    "embeddings_index = {}\n",
    "for k, v in wv_model.wv.vocab.items():\n",
    "    embeddings_index[k] = wv_model.wv[k]\n",
    "embedding_matrix = np.zeros(shape=(vocab_size+1, embedding_dim))\n",
    "for word, i in vocab.items():\n",
    "    if i > 10000: break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        if word == 'unk':\n",
    "            embedding_matrix[i] = np.average(list(embeddings_index.values()), axis=0)\n",
    "        else:\n",
    "            print(word, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    encoder_inputs = tf.placeholder( tf.int32, shape=(None, input_max_length), name='encoder_inputs')\n",
    "    decoder_inputs = tf.placeholder( tf.int32, shape=(None, output_max_length), name='decoder_inputs')\n",
    "    decoder_outputs = tf.placeholder( tf.int32, shape=(None, output_max_length), name='decoder_outputs')\n",
    "\n",
    "    source_sequence_length = tf.placeholder(tf.int32, shape=(None, ), name='encoder_lengths')\n",
    "    decoder_lengths = tf.placeholder(tf.int32, shape=(None,), name='decoder_lengths')\n",
    "\n",
    "    embeddings = tf.get_variable(\n",
    "        \"embeddings\", [vocab_size+1, embedding_dim]\n",
    "    )\n",
    "\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [vocab_size+1, embedding_dim])\n",
    "    embedding_init = embeddings.assign(embedding_placeholder)\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(embeddings, decoder_inputs)\n",
    "\n",
    "    with tf.name_scope(\"encoder_layer\"):\n",
    "        # Build RNN cell\n",
    "        forward_cell = tf.contrib.rnn.LSTMCell(latent_dim, name='forward_cell')\n",
    "        backward_cell = tf.contrib.rnn.LSTMCell(latent_dim, name='backward_cell')\n",
    "        forward_cell = tf.contrib.rnn.DropoutWrapper(forward_cell, output_keep_prob = keep_prob)\n",
    "        backward_cell = tf.contrib.rnn.DropoutWrapper(backward_cell, output_keep_prob = keep_prob)\n",
    "        \n",
    "        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_cell,\n",
    "            backward_cell,\n",
    "            inputs = encoder_emb_inp,\n",
    "            sequence_length = source_sequence_length,\n",
    "            time_major = False,\n",
    "            dtype = tf.float32\n",
    "        )\n",
    "        c = tf.concat([bi_state[0].c, bi_state[1].c], axis=1)\n",
    "        h = tf.concat([bi_state[0].h, bi_state[1].h], axis=1)\n",
    "        tf.summary.histogram('encoder_c', c)\n",
    "        tf.summary.histogram('encoder_h', h)\n",
    "        \n",
    "        encoder_outputs = tf.concat(bi_outputs, 2)\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(c, h)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('decoder_layer') as decoder_scope:\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(latent_dim * 2, name='decoder_cell')\n",
    "        decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, output_keep_prob = keep_prob)\n",
    "        projection_layer = tf.layers.Dense(vocab_size, use_bias=False)\n",
    "        \n",
    "        if attention == 'bahdanau':\n",
    "            bahdanau_attention = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                latent_dim * 2,\n",
    "                encoder_outputs,\n",
    "                memory_sequence_length= decoder_lengths,\n",
    "                normalize = True\n",
    "            )\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell,\n",
    "                bahdanau_attention,\n",
    "                alignment_history = True,\n",
    "                attention_layer_size = latent_dim * 2,\n",
    "                name = \"attention\"\n",
    "            )\n",
    "            decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "        elif attention == 'luong':\n",
    "            luong_attention = tf.contrib.seq2seq.LuongAttention(\n",
    "                latent_dim * 2,\n",
    "                encoder_outputs,\n",
    "                memory_sequence_length= decoder_lengths,\n",
    "            )\n",
    "            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                decoder_cell,\n",
    "                luong_attention,\n",
    "                alignment_history = True,\n",
    "                attention_layer_size = latent_dim * 2,\n",
    "                name = \"attention\"\n",
    "            )\n",
    "            decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "        else:\n",
    "            decoder_initial_state = encoder_state\n",
    "\n",
    "        # Helper\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            inputs = decoder_emb_inp,\n",
    "            sequence_length = decoder_lengths,\n",
    "            time_major = False\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = decoder_cell,\n",
    "            helper = helper,\n",
    "            initial_state = decoder_initial_state,\n",
    "            output_layer = projection_layer\n",
    "        )\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # Dynamic decoding\n",
    "        outputs, decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = decoder\n",
    "        )\n",
    "        if attention != None:\n",
    "            tf.summary.histogram('decoder_c', decoder_state.cell_state[0])\n",
    "            tf.summary.histogram('decoder_h', decoder_state.cell_state[1])\n",
    "            tf.summary.histogram('attention', decoder_state.attention)\n",
    "            attention_matrices = decoder_state.alignment_history.stack(name='train_atttention_matrix')\n",
    "        else:\n",
    "            tf.summary.histogram('decoder_c', decoder_state.c)\n",
    "            tf.summary.histogram('decoder_h', decoder_state.h)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = decoder_outputs,\n",
    "            logits = logits\n",
    "        )\n",
    "        non_zero = tf.cast(tf.not_equal(decoder_outputs, 0), tf.float32)\n",
    "        train_loss = (tf.reduce_sum(crossent * non_zero) / batch_size)\n",
    "        tf.summary.scalar('cross_entropy', train_loss)\n",
    "        \n",
    "    with tf.name_scope(\"train\"):\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(train_loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "        train_op = tf.train.AdamOptimizer(1e-3).apply_gradients(zip(clipped_gradients, params))\n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.name_scope(\"inference_layer\"):\n",
    "        num_sequences_to_decode = tf.placeholder(shape=(), dtype=tf.int32, name=\"num_seq\")\n",
    "        start_tokens = tf.tile([vocab['SOS']], [num_sequences_to_decode])\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embeddings,\n",
    "            start_tokens,\n",
    "            vocab['EOS']\n",
    "        )\n",
    "\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, inference_helper, decoder_initial_state,\n",
    "            output_layer = projection_layer\n",
    "        )\n",
    "        maximum_iterations = tf.round(tf.reduce_max(input_max_length))\n",
    "\n",
    "        greedy_decoding_result, greedy_final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            inference_decoder, maximum_iterations=maximum_iterations\n",
    "        )\n",
    "        if attention != None:\n",
    "            inference_attention_matrices = greedy_final_state.alignment_history.stack(\n",
    "                name=\"inference_attention_matrix\")\n",
    "        translations = greedy_decoding_result.sample_id\n",
    "\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(), graph=graph)\n",
    "sess.run(init)\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: embedding_matrix})\n",
    "\n",
    "#### tensorboard\n",
    "with graph.as_default():\n",
    "    writer = tf.summary.FileWriter('logdir/model_name')\n",
    "    writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss : 19.907690048217773\n",
      "iter 1280: loss : 20.95264434814453\n",
      "iter 2560: loss : 20.716644287109375\n",
      "iter 3840: loss : 18.418872833251953\n",
      "iter 5120: loss : 33.53544616699219\n",
      "iter 6400: loss : 19.420324325561523\n",
      "iter 7680: loss : 23.82167625427246\n",
      "iter 8960: loss : 22.351322174072266\n",
      "iter 10240: loss : 21.43043327331543\n",
      "iter 11520: loss : 21.610748291015625\n",
      "iter 12800: loss : 23.894025802612305\n",
      "iter 14080: loss : 20.366056442260742\n",
      "iter 15360: loss : 18.024038314819336\n",
      "iter 16640: loss : 23.31865119934082\n",
      "iter 17920: loss : 20.91519546508789\n",
      "iter 19200: loss : 19.04947853088379\n",
      "iter 20480: loss : 21.345624923706055\n",
      "iter 21760: loss : 21.700475692749023\n",
      "iter 23040: loss : 24.240285873413086\n",
      "iter 24320: loss : 30.5662899017334\n",
      "iter 25600: loss : 25.48529052734375\n",
      "iter 26880: loss : 23.891647338867188\n",
      "iter 28160: loss : 20.30861473083496\n",
      "iter 29440: loss : 18.400169372558594\n",
      "iter 30720: loss : 20.843971252441406\n",
      "iter 32000: loss : 29.648603439331055\n",
      "iter 33280: loss : 20.395748138427734\n",
      "iter 34560: loss : 27.977373123168945\n",
      "iter 35840: loss : 17.86972427368164\n",
      "iter 37120: loss : 14.436200141906738\n",
      "iter 38400: loss : 20.475427627563477\n",
      "iter 39680: loss : 24.983034133911133\n",
      "iter 40960: loss : 27.302433013916016\n",
      "iter 42240: loss : 23.65309715270996\n",
      "iter 43520: loss : 17.826406478881836\n",
      "iter 44800: loss : 16.923095703125\n",
      "iter 46080: loss : 22.598033905029297\n",
      "iter 47360: loss : 32.5993766784668\n",
      "iter 48640: loss : 28.00094223022461\n",
      "iter 49920: loss : 20.23430633544922\n",
      "iter 51200: loss : 25.640058517456055\n",
      "iter 52480: loss : 19.110248565673828\n",
      "iter 53760: loss : 22.095487594604492\n",
      "iter 55040: loss : 19.890439987182617\n",
      "iter 56320: loss : 16.030010223388672\n",
      "iter 57600: loss : 19.962099075317383\n",
      "iter 58880: loss : 16.799137115478516\n",
      "iter 60160: loss : 16.96455192565918\n",
      "iter 61440: loss : 25.800827026367188\n",
      "iter 62720: loss : 24.556720733642578\n",
      "iter 64000: loss : 21.987585067749023\n",
      "iter 65280: loss : 19.910396575927734\n",
      "iter 66560: loss : 29.400775909423828\n",
      "iter 67840: loss : 15.521642684936523\n",
      "iter 69120: loss : 21.858091354370117\n",
      "iter 70400: loss : 22.734004974365234\n",
      "70528\n",
      "iter 71680: loss : 26.007381439208984\n",
      "iter 72960: loss : 24.991701126098633\n",
      "iter 74240: loss : 20.671262741088867\n",
      "iter 75520: loss : 21.37297821044922\n",
      "iter 76800: loss : 28.73055076599121\n",
      "iter 78080: loss : 18.571243286132812\n",
      "iter 79360: loss : 21.211454391479492\n",
      "iter 80640: loss : 26.23247718811035\n",
      "iter 81920: loss : 17.834186553955078\n",
      "iter 83200: loss : 18.474843978881836\n",
      "iter 84480: loss : 24.868452072143555\n",
      "iter 85760: loss : 17.78007698059082\n",
      "iter 87040: loss : 23.537002563476562\n",
      "iter 88320: loss : 19.1096248626709\n",
      "iter 89600: loss : 17.002174377441406\n",
      "iter 90880: loss : 25.166980743408203\n",
      "iter 92160: loss : 20.05898666381836\n",
      "iter 93440: loss : 19.920520782470703\n",
      "iter 94720: loss : 18.21225357055664\n",
      "iter 96000: loss : 17.22773551940918\n",
      "iter 97280: loss : 22.8796329498291\n",
      "iter 98560: loss : 21.288928985595703\n",
      "iter 99840: loss : 26.844934463500977\n",
      "iter 101120: loss : 18.254377365112305\n",
      "iter 102400: loss : 18.709081649780273\n",
      "iter 103680: loss : 17.212251663208008\n",
      "iter 104960: loss : 20.72511863708496\n",
      "iter 106240: loss : 28.370817184448242\n",
      "iter 107520: loss : 17.845102310180664\n",
      "iter 108800: loss : 20.762557983398438\n",
      "iter 110080: loss : 18.304363250732422\n",
      "iter 111360: loss : 25.269620895385742\n",
      "iter 112640: loss : 23.95028305053711\n",
      "iter 113920: loss : 22.50088882446289\n",
      "iter 115200: loss : 20.345932006835938\n",
      "iter 116480: loss : 20.891143798828125\n",
      "iter 117760: loss : 20.65131187438965\n",
      "iter 119040: loss : 20.433969497680664\n",
      "iter 120320: loss : 12.958797454833984\n",
      "iter 121600: loss : 16.098756790161133\n",
      "iter 122880: loss : 25.354530334472656\n",
      "iter 124160: loss : 19.117467880249023\n",
      "iter 125440: loss : 22.23757553100586\n",
      "iter 126720: loss : 16.78802490234375\n",
      "iter 128000: loss : 28.317766189575195\n",
      "iter 129280: loss : 21.973236083984375\n",
      "iter 130560: loss : 22.345190048217773\n",
      "iter 131840: loss : 24.624189376831055\n",
      "iter 133120: loss : 21.276458740234375\n",
      "iter 134400: loss : 23.913171768188477\n",
      "iter 135680: loss : 20.3524112701416\n",
      "iter 136960: loss : 21.65193748474121\n",
      "iter 138240: loss : 26.593730926513672\n",
      "iter 139520: loss : 21.956403732299805\n",
      "iter 140800: loss : 20.22832679748535\n",
      "iter 142080: loss : 16.19813346862793\n",
      "iter 143360: loss : 20.318954467773438\n",
      "iter 144640: loss : 23.867069244384766\n",
      "iter 145920: loss : 17.93185043334961\n",
      "iter 147200: loss : 22.046001434326172\n",
      "iter 148480: loss : 19.21300506591797\n",
      "iter 149760: loss : 31.48625946044922\n",
      "iter 151040: loss : 25.386672973632812\n",
      "iter 152320: loss : 24.043123245239258\n",
      "iter 153600: loss : 20.546234130859375\n",
      "iter 154880: loss : 21.172754287719727\n",
      "iter 156160: loss : 15.987542152404785\n",
      "iter 157440: loss : 18.611711502075195\n",
      "iter 158720: loss : 15.090117454528809\n",
      "iter 160000: loss : 20.615781784057617\n",
      "iter 161280: loss : 20.205110549926758\n",
      "iter 162560: loss : 27.635040283203125\n",
      "iter 163840: loss : 21.9493408203125\n",
      "iter 165120: loss : 22.81800079345703\n",
      "iter 166400: loss : 22.44614601135254\n",
      "iter 167680: loss : 20.006107330322266\n",
      "iter 168960: loss : 26.692569732666016\n",
      "iter 170240: loss : 24.20034408569336\n",
      "iter 171520: loss : 17.687152862548828\n",
      "iter 172800: loss : 18.679105758666992\n",
      "iter 174080: loss : 19.982749938964844\n",
      "iter 175360: loss : 19.473886489868164\n",
      "iter 176640: loss : 17.2056941986084\n",
      "iter 177920: loss : 18.357498168945312\n",
      "iter 179200: loss : 16.160770416259766\n",
      "iter 180480: loss : 20.1961669921875\n",
      "iter 181760: loss : 19.97518539428711\n",
      "iter 183040: loss : 28.981647491455078\n",
      "iter 184320: loss : 27.307239532470703\n",
      "iter 185600: loss : 23.563987731933594\n",
      "iter 186880: loss : 17.547130584716797\n",
      "iter 188160: loss : 20.852046966552734\n",
      "iter 189440: loss : 23.219989776611328\n",
      "iter 190720: loss : 24.65342140197754\n",
      "iter 192000: loss : 23.55649757385254\n",
      "iter 193280: loss : 19.93000602722168\n",
      "iter 194560: loss : 17.591156005859375\n",
      "iter 195840: loss : 17.431270599365234\n",
      "iter 197120: loss : 22.750253677368164\n",
      "iter 198400: loss : 25.506149291992188\n",
      "iter 199680: loss : 25.112255096435547\n",
      "iter 200960: loss : 20.004165649414062\n",
      "iter 202240: loss : 20.01650047302246\n",
      "iter 203520: loss : 23.67832374572754\n",
      "iter 204800: loss : 18.716594696044922\n",
      "iter 206080: loss : 23.11387062072754\n",
      "iter 207360: loss : 24.637222290039062\n",
      "iter 208640: loss : 24.267513275146484\n",
      "iter 209920: loss : 11.568999290466309\n",
      "iter 211200: loss : 23.453948974609375\n",
      "iter 212480: loss : 24.23519515991211\n",
      "iter 213760: loss : 19.530193328857422\n",
      "iter 215040: loss : 20.185070037841797\n",
      "iter 216320: loss : 18.436681747436523\n",
      "iter 217600: loss : 24.893901824951172\n",
      "iter 218880: loss : 25.123315811157227\n",
      "iter 220160: loss : 19.593957901000977\n",
      "iter 221440: loss : 20.05742835998535\n",
      "iter 222720: loss : 26.909381866455078\n",
      "iter 224000: loss : 20.562849044799805\n",
      "iter 225280: loss : 18.729686737060547\n",
      "iter 226560: loss : 24.628273010253906\n",
      "iter 227840: loss : 25.89217185974121\n",
      "iter 229120: loss : 22.29673957824707\n",
      "iter 230400: loss : 24.34000587463379\n",
      "iter 231680: loss : 21.611684799194336\n",
      "iter 232960: loss : 20.081743240356445\n",
      "iter 234240: loss : 19.618921279907227\n",
      "iter 235520: loss : 17.49661636352539\n",
      "iter 236800: loss : 22.178909301757812\n",
      "iter 238080: loss : 19.39556121826172\n",
      "iter 239360: loss : 19.312084197998047\n",
      "iter 240640: loss : 21.439197540283203\n",
      "iter 241920: loss : 24.15608787536621\n",
      "iter 243200: loss : 26.72301483154297\n",
      "iter 244480: loss : 20.874004364013672\n",
      "iter 245760: loss : 19.411338806152344\n",
      "246656\n",
      "iter 247040: loss : 19.2782039642334\n",
      "iter 248320: loss : 18.81967544555664\n",
      "iter 249600: loss : 19.196516036987305\n",
      "iter 250880: loss : 22.54230499267578\n",
      "iter 252160: loss : 22.206872940063477\n",
      "iter 253440: loss : 24.051368713378906\n",
      "iter 254720: loss : 30.107542037963867\n",
      "iter 256000: loss : 28.439842224121094\n",
      "iter 257280: loss : 30.003705978393555\n",
      "iter 258560: loss : 20.344879150390625\n",
      "iter 259840: loss : 18.275558471679688\n",
      "iter 261120: loss : 18.941295623779297\n",
      "iter 262400: loss : 20.638547897338867\n",
      "iter 263680: loss : 16.336132049560547\n",
      "iter 264960: loss : 19.942920684814453\n",
      "iter 266240: loss : 19.449491500854492\n",
      "iter 267520: loss : 18.453811645507812\n",
      "iter 268800: loss : 22.64108657836914\n",
      "iter 270080: loss : 23.286897659301758\n",
      "iter 271360: loss : 19.00486183166504\n",
      "iter 272640: loss : 20.663835525512695\n",
      "iter 273920: loss : 19.5290584564209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 275200: loss : 25.865367889404297\n",
      "iter 276480: loss : 24.958187103271484\n",
      "iter 277760: loss : 18.680038452148438\n",
      "iter 279040: loss : 25.594371795654297\n",
      "iter 280320: loss : 18.2983455657959\n",
      "iter 281600: loss : 20.347461700439453\n",
      "iter 282880: loss : 24.58647346496582\n",
      "iter 284160: loss : 22.84800148010254\n",
      "iter 285440: loss : 20.63949966430664\n",
      "iter 286720: loss : 23.919050216674805\n",
      "iter 288000: loss : 20.526084899902344\n",
      "iter 289280: loss : 17.681901931762695\n",
      "iter 290560: loss : 25.131240844726562\n",
      "iter 291840: loss : 15.68746280670166\n",
      "iter 293120: loss : 22.05475616455078\n",
      "iter 294400: loss : 24.686447143554688\n",
      "iter 295680: loss : 20.526996612548828\n",
      "iter 296960: loss : 20.086393356323242\n",
      "iter 298240: loss : 17.09345817565918\n",
      "iter 299520: loss : 24.50650405883789\n",
      "iter 300800: loss : 15.113102912902832\n",
      "iter 302080: loss : 17.177833557128906\n",
      "iter 303360: loss : 20.61672592163086\n",
      "iter 304640: loss : 18.851512908935547\n",
      "iter 305920: loss : 25.729209899902344\n",
      "iter 307200: loss : 22.27849006652832\n",
      "iter 308480: loss : 21.683231353759766\n",
      "iter 309760: loss : 15.939128875732422\n",
      "iter 311040: loss : 21.08264923095703\n",
      "iter 312320: loss : 23.15115737915039\n",
      "iter 313600: loss : 21.372697830200195\n",
      "iter 314880: loss : 14.854349136352539\n",
      "iter 316160: loss : 14.42209529876709\n",
      "iter 317440: loss : 19.441688537597656\n",
      "iter 318720: loss : 18.512941360473633\n",
      "iter 320000: loss : 20.89232063293457\n",
      "iter 321280: loss : 18.2557373046875\n",
      "iter 322560: loss : 22.34963035583496\n",
      "iter 323840: loss : 21.50275230407715\n",
      "iter 325120: loss : 14.883865356445312\n",
      "iter 326400: loss : 20.555395126342773\n",
      "iter 327680: loss : 22.983001708984375\n",
      "iter 328960: loss : 22.54515838623047\n",
      "iter 330240: loss : 23.505037307739258\n",
      "iter 331520: loss : 19.583938598632812\n",
      "iter 332800: loss : 20.79670524597168\n",
      "iter 334080: loss : 21.008745193481445\n",
      "iter 335360: loss : 18.59539222717285\n",
      "iter 336640: loss : 17.779987335205078\n",
      "iter 337920: loss : 29.56314468383789\n",
      "iter 339200: loss : 20.10401153564453\n",
      "iter 340480: loss : 20.474069595336914\n",
      "iter 341760: loss : 23.22199249267578\n",
      "iter 343040: loss : 24.6964111328125\n",
      "iter 344320: loss : 21.174304962158203\n",
      "iter 345600: loss : 25.066434860229492\n",
      "iter 346880: loss : 22.734132766723633\n",
      "iter 348160: loss : 19.316776275634766\n",
      "iter 349440: loss : 22.31124496459961\n",
      "iter 350720: loss : 21.706544876098633\n",
      "iter 352000: loss : 28.962722778320312\n",
      "iter 353280: loss : 18.756397247314453\n",
      "iter 354560: loss : 31.18524169921875\n",
      "iter 355840: loss : 25.508012771606445\n",
      "iter 357120: loss : 23.357297897338867\n",
      "iter 358400: loss : 21.076824188232422\n",
      "iter 359680: loss : 16.45667266845703\n",
      "iter 360960: loss : 22.306304931640625\n",
      "iter 362240: loss : 19.732770919799805\n",
      "iter 363520: loss : 20.53143310546875\n",
      "iter 364800: loss : 22.18054962158203\n",
      "iter 366080: loss : 19.89804458618164\n",
      "iter 367360: loss : 15.679530143737793\n",
      "iter 368640: loss : 25.924945831298828\n",
      "iter 369920: loss : 17.969858169555664\n",
      "iter 371200: loss : 21.692672729492188\n",
      "iter 372480: loss : 17.554033279418945\n",
      "iter 373760: loss : 28.109054565429688\n",
      "iter 375040: loss : 17.58312225341797\n",
      "iter 376320: loss : 37.398651123046875\n",
      "iter 377600: loss : 20.251346588134766\n",
      "iter 378880: loss : 20.743999481201172\n",
      "iter 380160: loss : 23.057344436645508\n",
      "iter 381440: loss : 21.14711570739746\n",
      "iter 382720: loss : 21.138872146606445\n",
      "iter 384000: loss : 23.568206787109375\n",
      "iter 385280: loss : 17.746061325073242\n",
      "iter 386560: loss : 23.776531219482422\n",
      "iter 387840: loss : 19.150453567504883\n",
      "iter 389120: loss : 20.17559242248535\n",
      "iter 390400: loss : 20.53057861328125\n",
      "iter 391680: loss : 19.144542694091797\n",
      "iter 392960: loss : 20.509357452392578\n",
      "iter 394240: loss : 25.648414611816406\n",
      "iter 395520: loss : 22.172334671020508\n",
      "iter 396800: loss : 17.892072677612305\n",
      "iter 398080: loss : 18.45410919189453\n",
      "iter 399360: loss : 23.05313491821289\n",
      "iter 400640: loss : 16.09644889831543\n",
      "iter 401920: loss : 28.137178421020508\n",
      "iter 403200: loss : 21.02944564819336\n",
      "iter 404480: loss : 19.189382553100586\n",
      "iter 405760: loss : 17.669002532958984\n",
      "iter 407040: loss : 25.300792694091797\n",
      "iter 408320: loss : 26.677173614501953\n",
      "iter 409600: loss : 19.589113235473633\n",
      "iter 410880: loss : 19.466289520263672\n",
      "iter 412160: loss : 20.960451126098633\n",
      "iter 413440: loss : 23.506603240966797\n",
      "iter 414720: loss : 31.920894622802734\n",
      "iter 416000: loss : 18.388273239135742\n",
      "iter 417280: loss : 16.9440860748291\n",
      "iter 418560: loss : 21.099021911621094\n",
      "iter 419840: loss : 20.010366439819336\n",
      "iter 421120: loss : 24.452495574951172\n",
      "iter 422400: loss : 17.115537643432617\n",
      "iter 423680: loss : 22.889989852905273\n",
      "iter 424960: loss : 14.14511775970459\n",
      "iter 426240: loss : 22.22420310974121\n",
      "iter 427520: loss : 27.27313232421875\n",
      "iter 428800: loss : 27.00775146484375\n",
      "iter 430080: loss : 26.8599796295166\n",
      "iter 431360: loss : 26.622766494750977\n",
      "iter 432640: loss : 20.268083572387695\n",
      "iter 433920: loss : 23.04465675354004\n",
      "iter 435200: loss : 22.13867950439453\n",
      "iter 436480: loss : 17.5196533203125\n",
      "iter 437760: loss : 24.8642578125\n",
      "iter 439040: loss : 20.62790298461914\n",
      "iter 440320: loss : 18.651630401611328\n",
      "iter 441600: loss : 18.627477645874023\n",
      "iter 442880: loss : 15.107355117797852\n",
      "iter 444160: loss : 18.832904815673828\n",
      "iter 445440: loss : 22.061763763427734\n",
      "iter 446720: loss : 19.380102157592773\n",
      "iter 448000: loss : 19.19163703918457\n",
      "iter 449280: loss : 18.963882446289062\n",
      "iter 450560: loss : 22.64817237854004\n",
      "iter 451840: loss : 22.08594512939453\n",
      "iter 453120: loss : 15.46824836730957\n",
      "iter 454400: loss : 16.27613067626953\n",
      "iter 455680: loss : 22.604984283447266\n",
      "iter 456960: loss : 19.443790435791016\n",
      "iter 458240: loss : 27.20680046081543\n",
      "iter 459520: loss : 19.701278686523438\n",
      "iter 460800: loss : 13.583609580993652\n",
      "iter 462080: loss : 22.91086769104004\n",
      "iter 463360: loss : 17.63643455505371\n",
      "iter 464640: loss : 22.086793899536133\n",
      "iter 465920: loss : 15.759754180908203\n",
      "iter 467200: loss : 26.66192626953125\n",
      "iter 468480: loss : 23.251890182495117\n",
      "iter 469760: loss : 31.29503631591797\n",
      "iter 471040: loss : 19.833433151245117\n",
      "iter 472320: loss : 15.276761054992676\n",
      "iter 473600: loss : 18.27655792236328\n",
      "iter 474880: loss : 22.959535598754883\n",
      "iter 476160: loss : 17.14761734008789\n",
      "iter 477440: loss : 14.02861499786377\n",
      "iter 478720: loss : 15.199543952941895\n",
      "iter 480000: loss : 19.337753295898438\n",
      "iter 481280: loss : 13.499174118041992\n",
      "iter 482560: loss : 24.108951568603516\n",
      "iter 483840: loss : 18.292734146118164\n",
      "iter 485120: loss : 18.158559799194336\n",
      "iter 486400: loss : 20.182125091552734\n",
      "iter 487680: loss : 15.212897300720215\n",
      "iter 488960: loss : 24.08127784729004\n",
      "iter 490240: loss : 18.64336395263672\n",
      "iter 491520: loss : 17.382354736328125\n",
      "iter 492800: loss : 17.6922550201416\n",
      "iter 494080: loss : 23.792095184326172\n",
      "iter 495360: loss : 23.262229919433594\n",
      "iter 496640: loss : 23.089664459228516\n",
      "iter 497920: loss : 18.548002243041992\n",
      "iter 499200: loss : 25.009288787841797\n",
      "iter 500480: loss : 17.333515167236328\n",
      "iter 501760: loss : 28.949054718017578\n",
      "iter 503040: loss : 19.733957290649414\n",
      "iter 504320: loss : 16.718032836914062\n",
      "iter 505600: loss : 19.038820266723633\n",
      "iter 506880: loss : 25.128631591796875\n",
      "iter 508160: loss : 16.843608856201172\n",
      "iter 509440: loss : 16.44672966003418\n",
      "iter 510720: loss : 23.691612243652344\n",
      "iter 512000: loss : 25.43118667602539\n",
      "iter 513280: loss : 16.141651153564453\n",
      "iter 514560: loss : 15.93634033203125\n",
      "iter 515840: loss : 19.9952335357666\n",
      "iter 517120: loss : 22.9603328704834\n",
      "iter 518400: loss : 24.866397857666016\n",
      "iter 519680: loss : 19.29296112060547\n",
      "iter 520960: loss : 22.305932998657227\n",
      "iter 522240: loss : 22.56608009338379\n",
      "iter 523520: loss : 23.65314483642578\n",
      "iter 524800: loss : 22.531278610229492\n",
      "iter 526080: loss : 25.91128921508789\n",
      "iter 527360: loss : 18.468355178833008\n",
      "iter 528640: loss : 18.99176597595215\n",
      "iter 529920: loss : 27.522489547729492\n",
      "iter 531200: loss : 19.868576049804688\n",
      "iter 532480: loss : 28.133506774902344\n",
      "iter 533760: loss : 15.859062194824219\n",
      "iter 535040: loss : 17.352415084838867\n",
      "iter 536320: loss : 18.97733497619629\n",
      "iter 537600: loss : 22.406232833862305\n",
      "iter 538880: loss : 22.04430389404297\n",
      "iter 540160: loss : 20.339162826538086\n",
      "iter 541440: loss : 18.03870391845703\n",
      "iter 542720: loss : 22.716190338134766\n",
      "iter 544000: loss : 20.181766510009766\n",
      "iter 545280: loss : 14.38233757019043\n",
      "iter 546560: loss : 19.289813995361328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 547840: loss : 16.01066017150879\n",
      "iter 549120: loss : 14.277397155761719\n",
      "iter 550400: loss : 20.370216369628906\n",
      "iter 551680: loss : 15.227815628051758\n",
      "iter 552960: loss : 19.64551544189453\n",
      "iter 554240: loss : 22.293949127197266\n",
      "iter 555520: loss : 18.676189422607422\n",
      "iter 556800: loss : 15.606348037719727\n",
      "iter 558080: loss : 24.892988204956055\n",
      "iter 559360: loss : 20.186954498291016\n",
      "iter 560640: loss : 20.98318099975586\n",
      "iter 561920: loss : 17.635404586791992\n",
      "iter 563200: loss : 18.327362060546875\n",
      "iter 564480: loss : 15.868371963500977\n",
      "iter 565760: loss : 22.697500228881836\n",
      "iter 567040: loss : 24.749649047851562\n",
      "iter 568320: loss : 18.068479537963867\n",
      "iter 569600: loss : 21.01357078552246\n",
      "iter 570880: loss : 20.845458984375\n",
      "iter 572160: loss : 23.69152069091797\n",
      "iter 573440: loss : 21.76560401916504\n",
      "iter 574720: loss : 15.020743370056152\n",
      "iter 576000: loss : 28.323301315307617\n",
      "iter 577280: loss : 19.20453643798828\n",
      "iter 578560: loss : 17.44856071472168\n",
      "iter 579840: loss : 17.161550521850586\n",
      "iter 581120: loss : 24.502445220947266\n",
      "iter 582400: loss : 24.690170288085938\n",
      "iter 583680: loss : 23.17552947998047\n",
      "iter 584960: loss : 17.033721923828125\n",
      "iter 586240: loss : 37.87590026855469\n",
      "iter 587520: loss : 17.91902732849121\n",
      "iter 588800: loss : 20.146862030029297\n",
      "iter 590080: loss : 21.880529403686523\n",
      "iter 591360: loss : 18.45038414001465\n",
      "iter 592640: loss : 19.40888023376465\n",
      "iter 593920: loss : 28.08783721923828\n",
      "iter 595200: loss : 22.162504196166992\n",
      "iter 596480: loss : 16.65398597717285\n",
      "iter 597760: loss : 15.259136199951172\n",
      "iter 599040: loss : 24.284563064575195\n",
      "iter 600320: loss : 17.83557891845703\n",
      "iter 601600: loss : 22.656587600708008\n",
      "iter 602880: loss : 18.512367248535156\n",
      "iter 604160: loss : 16.746397018432617\n",
      "iter 605440: loss : 20.98141860961914\n",
      "iter 606720: loss : 29.913331985473633\n",
      "iter 608000: loss : 17.120010375976562\n",
      "iter 609280: loss : 13.332776069641113\n",
      "iter 610560: loss : 17.568429946899414\n",
      "iter 611840: loss : 18.100099563598633\n",
      "iter 613120: loss : 12.241816520690918\n",
      "iter 614400: loss : 20.803985595703125\n",
      "iter 615680: loss : 17.368322372436523\n",
      "iter 616960: loss : 22.800952911376953\n",
      "iter 618240: loss : 11.544809341430664\n",
      "iter 619520: loss : 16.03620719909668\n",
      "iter 620800: loss : 19.639976501464844\n",
      "iter 622080: loss : 17.89364242553711\n",
      "iter 623360: loss : 16.282419204711914\n",
      "iter 624640: loss : 18.791584014892578\n",
      "iter 625920: loss : 18.941699981689453\n",
      "iter 627200: loss : 22.262767791748047\n",
      "iter 628480: loss : 19.576738357543945\n",
      "iter 629760: loss : 20.191078186035156\n",
      "iter 631040: loss : 20.134504318237305\n",
      "iter 632320: loss : 19.138648986816406\n",
      "iter 633600: loss : 23.21862030029297\n",
      "iter 634880: loss : 14.996953964233398\n",
      "iter 636160: loss : 20.809425354003906\n",
      "iter 637440: loss : 19.719141006469727\n",
      "iter 638720: loss : 16.459388732910156\n",
      "iter 640000: loss : 17.184856414794922\n",
      "iter 641280: loss : 17.79087257385254\n",
      "iter 642560: loss : 26.228336334228516\n",
      "iter 643840: loss : 20.610286712646484\n",
      "iter 645120: loss : 18.41750717163086\n",
      "iter 646400: loss : 18.58479118347168\n",
      "iter 647680: loss : 20.640295028686523\n",
      "iter 648960: loss : 15.939233779907227\n",
      "iter 650240: loss : 16.3432674407959\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [6,256] vs. shape[1] = [128,512]\n\t [[Node: loss/decoder/while/BasicDecoderStep/decoder/attention/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/decoder/while/Switch_10:1, loss/decoder/while/Switch_5:1, loss/decoder/while/BasicDecoderStep/decoder/attention/concat/axis)]]\n\t [[Node: loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/_48 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_107_loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add)]]\n\nCaused by op 'loss/decoder/while/BasicDecoderStep/decoder/attention/concat', defined at:\n  File \"c:\\users\\waynet\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\waynet\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-110-95171f12ae02>\", line 101, in <module>\n    decoder = decoder\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3202, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2940, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2877, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 254, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\basic_decoder.py\", line 138, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 191, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py\", line 1376, in call\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py\", line 1178, in <lambda>\n    lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1181, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1101, in concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [6,256] vs. shape[1] = [128,512]\n\t [[Node: loss/decoder/while/BasicDecoderStep/decoder/attention/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/decoder/while/Switch_10:1, loss/decoder/while/Switch_5:1, loss/decoder/while/BasicDecoderStep/decoder/attention/concat/axis)]]\n\t [[Node: loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/_48 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_107_loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [6,256] vs. shape[1] = [128,512]\n\t [[Node: loss/decoder/while/BasicDecoderStep/decoder/attention/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/decoder/while/Switch_10:1, loss/decoder/while/Switch_5:1, loss/decoder/while/BasicDecoderStep/decoder/attention/concat/axis)]]\n\t [[Node: loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/_48 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_107_loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-febac310eeb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"iter {}: loss : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mloss_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epcoh {}: loss : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [6,256] vs. shape[1] = [128,512]\n\t [[Node: loss/decoder/while/BasicDecoderStep/decoder/attention/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/decoder/while/Switch_10:1, loss/decoder/while/Switch_5:1, loss/decoder/while/BasicDecoderStep/decoder/attention/concat/axis)]]\n\t [[Node: loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/_48 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_107_loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add)]]\n\nCaused by op 'loss/decoder/while/BasicDecoderStep/decoder/attention/concat', defined at:\n  File \"c:\\users\\waynet\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\waynet\\appdata\\local\\programs\\python\\python36\\Lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-110-95171f12ae02>\", line 101, in <module>\n    decoder = decoder\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 309, in dynamic_decode\n    swap_memory=swap_memory)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3202, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2940, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2877, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\decoder.py\", line 254, in body\n    decoder_finished) = decoder.step(time, inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\basic_decoder.py\", line 138, in step\n    cell_outputs, cell_state = self._cell(inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 191, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py\", line 1376, in call\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\contrib\\seq2seq\\python\\ops\\attention_wrapper.py\", line 1178, in <lambda>\n    lambda inputs, attention: array_ops.concat([inputs, attention], -1))\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1181, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1101, in concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"c:\\users\\waynet\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): ConcatOp : Dimensions of inputs should match: shape[0] = [6,256] vs. shape[1] = [128,512]\n\t [[Node: loss/decoder/while/BasicDecoderStep/decoder/attention/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/decoder/while/Switch_10:1, loss/decoder/while/Switch_5:1, loss/decoder/while/BasicDecoderStep/decoder/attention/concat/axis)]]\n\t [[Node: loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add/_48 = _Send[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_107_loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](loss/decoder/while/BasicDecoderStep/TrainingHelperNextInputs/add)]]\n"
     ]
    }
   ],
   "source": [
    "#### training process\n",
    "\n",
    "epochs = 2\n",
    "shuffle_index = np.array(range(0, len(src_data)))\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(src_data), batch_size):\n",
    "        index_batch = shuffle_index[i:i + batch_size]\n",
    "        train_out = np.array(list(map(lambda x: x[1:], tar_data[index_batch])))\n",
    "        train_tar = np.array(list(map(lambda x: x[:-1], tar_data[index_batch])))\n",
    "\n",
    "        train_src_length = src_lengths[index_batch]\n",
    "        train_tar_length = np.array([25] * len(index_batch))\n",
    "        \n",
    "        train_src = np.array(pad_sequences(src_data[index_batch], input_max_length, padding='post'))\n",
    "        train_tar = np.array(pad_sequences(train_tar, output_max_length, padding='post'))\n",
    "        train_out = np.array(pad_sequences(train_out, output_max_length, padding='post'))\n",
    "\n",
    "        feed_dict = {\n",
    "            encoder_inputs: train_src,\n",
    "            decoder_inputs: train_tar,\n",
    "            decoder_outputs: train_out,\n",
    "            source_sequence_length: train_src_length,\n",
    "            decoder_lengths: train_tar_length\n",
    "        }\n",
    "        if index_batch.shape[0] != batch_size:\n",
    "            continue\n",
    "        loss_epoch = sess.run(train_loss, feed_dict)\n",
    "        if (math.isnan(loss_epoch)):\n",
    "            print(i)\n",
    "        else:\n",
    "            sess.run(train_op, feed_dict)\n",
    "        if i % 1280 == 0:\n",
    "            s = sess.run(merged_summary, feed_dict)\n",
    "            writer.add_summary(s, epoch * len(src_data) + i)\n",
    "            print(\"iter {}: loss : {}\".format(i, loss_epoch))\n",
    "\n",
    "    loss_epoch = sess.run(train_loss, feed_dict)\n",
    "    print(\"epcoh {}: loss : {}\".format(epoch, loss_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: model/luong_attention_4.ckpt\n"
     ]
    }
   ],
   "source": [
    "#### model saver\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "    print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/luong_attention_4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/luong_attention_4.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "#### testing\n",
    "\n",
    "test_data = list(map(lambda x: to_sequence(x.split(' '), vocab), test[0].values))\n",
    "test_lengths = list(map(lambda x: len(x), test_data))\n",
    "\n",
    "with open('output.csv', 'w', encoding='utf-8') as f,tf.Session(config=tf.ConfigProto(), graph=graph) as sess:\n",
    "    saver = tf.train.Saver()    \n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    \n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        test_data_batch = np.array(pad_sequences(test_data[ i: i + batch_size ], input_max_length, padding='post'))\n",
    "        test_lengths_batch =  test_lengths[ i: i + batch_size ]\n",
    "        test_batch_size = len(test_lengths_batch)\n",
    "        \n",
    "        if test_batch_size == 0:\n",
    "            continue\n",
    "        elif test_batch_size != batch_size:\n",
    "            padding = np.zeros(shape=(batch_size - test_batch_size, input_max_length), dtype=np.int32)\n",
    "            test_data_batch = np.concatenate((test_data_batch, padding), axis=0)\n",
    "            padding = np.zeros(shape=(batch_size - test_batch_size), dtype=np.int32)\n",
    "            test_lengths_batch = np.concatenate((test_lengths_batch, padding), axis=0)\n",
    "                               \n",
    "            feed_dict = {\n",
    "                encoder_inputs: test_data_batch,\n",
    "                source_sequence_length: test_lengths_batch,\n",
    "                decoder_lengths: np.array([25] * batch_size),\n",
    "                num_sequences_to_decode: batch_size\n",
    "            }\n",
    "            replies = sess.run(translations, feed_dict=feed_dict)[:test_batch_size, :]\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                encoder_inputs: test_data_batch,\n",
    "                source_sequence_length: test_lengths_batch,\n",
    "                decoder_lengths: np.array([25] * batch_size),\n",
    "                num_sequences_to_decode: batch_size\n",
    "            }\n",
    "            replies = sess.run(translations, feed_dict=feed_dict)\n",
    "        replies = list(map(lambda x: to_sentence(x, reverse_vocab), replies))\n",
    "        replies = list(map(lambda x: ' '.join(x), replies))\n",
    "        for result in replies:\n",
    "            f.write('{}\\n'.format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
