{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hw2 task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ntucsie-sdml2018-2-1/rating_train.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('unique userid:', len(dataset.userid.unique()))\n",
    "print('unique foodid:', len(dataset.foodid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = dataset.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in matrix:\n",
    "    freq[row[1]][row[2]] = freq[row[1]][row[2]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for userid in freq:\n",
    "    for foodid in freq[userid]:\n",
    "        train.append([userid, foodid, freq[userid][foodid]])\n",
    "train = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['userid'] = train[:,0]\n",
    "df['foodid'] = train[:,1]\n",
    "df['freq'] = train[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_food = dataset['foodid'].max()\n",
    "n_neg_sample = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('food.train.rating', 'w') as file:\n",
    "    for index, row in dataset.iterrows():\n",
    "        timestamp = int(datetime.datetime.strptime(row['date'], '%Y-%m-%d').strftime(\"%s\"))\n",
    "        file.write('{}\\t{}\\t1\\t{}\\n'.format(row['userid'], row['foodid'], timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('food.test.rating', 'w') as file, open('food.test.negative', 'w') as neg_file:\n",
    "    for i in range(dataset['userid'].max()):\n",
    "        tmp_df = dataset[dataset['userid'] == i]\n",
    "        if tmp_df.shape[0] > 0:\n",
    "            sample = tmp_df.sample().values[0]\n",
    "            timestamp = int(datetime.datetime.strptime(sample[0], '%Y-%m-%d').strftime(\"%s\"))\n",
    "            file.write('{}\\t{}\\t1\\t{}\\n'.format(sample[1], sample[2], timestamp))\n",
    "            \n",
    "            # Negative sampling\n",
    "            cnt = 0\n",
    "            neg_file.write('({},{})'.format(sample[1], sample[2]))\n",
    "            while (cnt < n_neg_sample):\n",
    "                n_foodid = np.random.randint(n_food+1)\n",
    "                if n_foodid not in history[sample[1]]:\n",
    "                    neg_file.write('\\t{}'.format(n_foodid))\n",
    "                    cnt = cnt + 1\n",
    "            neg_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = pd.read_csv('ntucsie-sdml2018-2-1/food.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_filter = food[['foodid', \n",
    "                    'calories', \n",
    "                    'fat', \n",
    "                    'carbs', \n",
    "                    'sodium', \n",
    "                    'potassium', \n",
    "                    'fiber', \n",
    "                    'sugar', \n",
    "                    'protein', \n",
    "                    'calcium', \n",
    "                    'iron']]\n",
    "food_filter = food_filter.replace('-', 0)\n",
    "food_v = food_filter.astype(np.int).get_values()\n",
    "foodmap = {row[0]: row[1:] for row in food_v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv('ntucsie-sdml2018-2-1/user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_filter = user[['userid', 'age', 'gender', 'location', 'friends_count']]\n",
    "user_filter = user_filter.replace(np.NAN, 0)\n",
    "user_one_hot = pd.get_dummies(user_filter, columns=['gender', 'location'])\n",
    "user_v = user_one_hot.astype(np.int).get_values()\n",
    "usermap = {row[0]: row[1:] for row in user_v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('foodmap.pickle', 'wb') as f_file, open('usermap.pickle', 'wb') as u_file:\n",
    "    pickle.dump(foodmap, f_file)\n",
    "    pickle.dump(usermap, u_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.groupby('userid').count().index.tolist()\n",
    "y = dataset.groupby('userid').count()['foodid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.describe(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x, y)\n",
    "plt.title('User histogram')\n",
    "plt.xlabel('user id')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.groupby('foodid').count().index.tolist()\n",
    "y = dataset.groupby('foodid').count()['userid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.describe(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x, y, log=True)\n",
    "plt.title('Food histogram')\n",
    "plt.xlabel('food id')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y).argsort()\n",
    "print(y[4769])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(food.loc[food['foodid'] == 4769])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(dataset.userid.unique()), dataset.foodid.max()+1), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2userid = dataset.userid.unique()\n",
    "userid2index = defaultdict(int)\n",
    "for index, userid in enumerate(index2userid):\n",
    "    userid2index[userid] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for userid in freq:\n",
    "    for foodid in freq[userid]:\n",
    "        # Implicit feedback\n",
    "        if freq[userid][foodid] > 0:\n",
    "            X[userid2index[userid]][foodid] = 1\n",
    "        else:\n",
    "            X[userid2index[userid]][foodid] = 0\n",
    "        # Explicit feedback\n",
    "#         X[userid2index[userid]][foodid] = freq[userid][foodid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, TruncatedSVD, PCA\n",
    "\n",
    "# model = NMF(n_components=20, init='random', random_state=0)\n",
    "# model = TruncatedSVD(n_components=20, n_iter=100, random_state=0)\n",
    "model = PCA(n_components=20, random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred.csv', 'w') as file:\n",
    "    file.write('userid,foodid\\n')\n",
    "    for index, row in enumerate(pred):\n",
    "        userid = index2userid[index]\n",
    "        if userid not in freq:\n",
    "            continue\n",
    "        file.write('{},'.format(userid))\n",
    "        row_sorted = row.argsort()[::-1]\n",
    "        # row_sorted.sort() # foodid from small to large\n",
    "        cnt = 0\n",
    "        for foodid in row_sorted:\n",
    "            if foodid not in freq[userid]:\n",
    "                file.write('{}'.format(foodid))\n",
    "                cnt = cnt + 1\n",
    "                if cnt >= 20:\n",
    "                    file.write('\\n')\n",
    "                    break\n",
    "                else:\n",
    "                    file.write(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['userid'] = df['userid'].astype('category')\n",
    "df['foodid'] = df['foodid'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence\n",
    "alpha = 10\n",
    "confidence = 1 + alpha * df['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = df['freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_data = coo_matrix((confidence.astype(int),\n",
    "                   (df['foodid'].cat.codes,\n",
    "                    df['userid'].cat.codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2user = dict(enumerate(df['userid'].cat.categories))\n",
    "user2index = dict((user, index) for index, user in index2user.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2food = dict(enumerate(df['foodid'].cat.categories))\n",
    "food2index = dict((user, index) for index, user in index2user.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df.loc[df['userid'] == 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50,\n",
    "                                             iterations=1000,\n",
    "                                             calculate_training_loss=True)\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "model.fit(item_user_data)\n",
    "\n",
    "# recommend items for a user\n",
    "userid = 0\n",
    "user_items = item_user_data.T.tocsr()\n",
    "recommendations = model.recommend(userid, user_items, N=20)\n",
    "\n",
    "# find related items\n",
    "itemid = 0\n",
    "related = model.similar_items(itemid, N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Personalized Ranking (BPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize a model\n",
    "model = implicit.bpr.BayesianPersonalizedRanking(factors=10, iterations=2000)\n",
    "\n",
    "# train the model on a sparse matrix of item/user/confidence weights\n",
    "model.fit(item_user_data)\n",
    "\n",
    "# recommend items for a user\n",
    "userid = 0\n",
    "user_items = item_user_data.T.tocsr()\n",
    "recommendations = model.recommend(userid, user_items, N=20)\n",
    "\n",
    "# find related items\n",
    "related = model.similar_items(itemid, N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodfreq = dataset.groupby('foodid').count()['userid']\n",
    "foodfreq = foodfreq.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = defaultdict(set)\n",
    "for index, row in df.iterrows():\n",
    "    history[row['userid']].add(row['foodid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('history.pickle', 'wb') as file:\n",
    "    pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pred.csv', 'w') as file:\n",
    "    file.write('userid,foodid\\n')\n",
    "    total_cnt = 0\n",
    "    total_score = 0\n",
    "    for index, userid in index2user.items():\n",
    "        file.write('{},'.format(userid))\n",
    "        recommendations = model.recommend(index, user_items, N=20)\n",
    "        cnt = 0\n",
    "        for (foodid, score) in recommendations:\n",
    "            if cnt < 10:\n",
    "                for index, freq in foodfreq.iteritems():\n",
    "                    if index not in history[userid]:\n",
    "                        history[userid].add(index)\n",
    "                        foodid = index\n",
    "                        break\n",
    "            file.write('{}'.format(foodid))\n",
    "            total_cnt = total_cnt + 1\n",
    "            total_score = total_score + score\n",
    "            cnt = cnt + 1\n",
    "            if cnt >= 20:\n",
    "                file.write('\\n')\n",
    "            else:\n",
    "                file.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = total_score / total_cnt\n",
    "print('Average score:', avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURPRISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1, 4], [2, 5], [3, 6]], columns=['user', 'item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_items.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import Concatenate, Multiply\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users, num_items, num_user_attrs, num_item_attrs, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    user_attr = Input(shape=(num_user_attrs,), name='user_attr')\n",
    "    item_attr = Input(shape=(num_item_attrs,), name='item_attr')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  embeddings_initializer = 'glorot_normal')\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  embeddings_initializer = 'glorot_normal')   \n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]//2, name = \"mlp_embedding_user\",\n",
    "                                  embeddings_initializer = 'glorot_normal')\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]//2, name = 'mlp_embedding_item',\n",
    "                                  embeddings_initializer = 'glorot_normal')   \n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent]) # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent, user_attr, item_attr])\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "    #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input, user_attr, item_attr], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(9895, 5532, len(usermap[6]), len(foodmap[6]), layers=[512, 256, 128, 64], reg_layers=[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=True, rankdir=\"HB\").create(prog=\"dot\", format=\"svg\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
